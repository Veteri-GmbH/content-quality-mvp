# Feature Plan: URL-Limit und verbesserte Progress-Anzeige

## Kontext
User berichtet, dass der Audit-Progress bei 1 Seite stehen bleibt und nicht weitergeht. Außerdem sieht man nicht, welche Seiten fertig sind. Zusätzlich soll beim Setup der Sitemap eine Auswahl hinzugefügt werden, wie viele URLs gecrawlt werden sollen (z.B. 10, 50, 100, etc.).

---

## Problem 1: Job-Processor läuft möglicherweise nicht korrekt

### Analyse
- Der Job-Processor in `server/src/services/job-queue.ts` verarbeitet Jobs sequenziell
- Nach einem Job wird `processNext()` rekursiv aufgerufen
- Problem: Wenn ein Job fehlschlägt oder hängt, blockiert er die gesamte Queue
- Der Job-Processor verwendet nur einen Worker-Thread

### Lösung: Multi-Worker Job-Processor

**Datei:** `server/src/services/job-queue.ts`

1. **Änderung der `startJobProcessor` Funktion:**
   - Starte mehrere Worker parallel (z.B. 3-5 Worker)
   - Jeder Worker läuft in seiner eigenen Schleife
   - Worker können unabhängig voneinander Jobs verarbeiten

2. **Rate-Limiting pro Audit:**
   - Füge Logik hinzu, die sicherstellt, dass pro Audit nur 1 Crawl-Job gleichzeitig läuft
   - Erlaube mehrere Analyze-Jobs parallel
   - Speichere aktive Crawl-Jobs pro Audit in Memory-Map

3. **Besseres Error-Handling:**
   - Stelle sicher, dass failed Jobs nicht die Queue blockieren
   - Logge fehlgeschlagene Jobs detailliert

---

## Problem 2: UI zeigt nicht, welche Seiten fertig sind

### Analyse
- Die AuditTable-Komponente zeigt die Seiten an
- Der Status wird nicht visuell hervorgehoben
- Sortierung ist unklar

### Lösung: Verbesserte Seiten-Tabelle

**Datei:** `ui/src/components/audit-table.tsx`

1. **Status-Spalte hinzufügen:**
   - Zeige Status-Badge für jede Seite (pending, crawling, analyzing, completed, failed)
   - Verwende farbcodierte Badges ähnlich wie in `AuditDetail.tsx`

2. **Sortierung:**
   - Zeige completed/failed Seiten oben
   - Pending/crawling/analyzing Seiten darunter
   - Frontend-Sortierung nach Status

3. **Live-Updates:**
   - In `AuditDetail.tsx` wird bereits alle 5 Sekunden refreshed
   - Die Tabelle wird dadurch automatisch aktualisiert

---

## Feature 3: URL-Limit Auswahl

### Datei-Änderungen

#### 1. Schema-Änderung: `server/src/schema/audits.ts`
- Füge Feld `url_limit` zur `audits` Tabelle hinzu:
  ```typescript
  url_limit: integer('url_limit'), // null = alle URLs
  ```
- Erstelle Migration: `server/drizzle/0003_url_limit.sql`

#### 2. API-Änderung: `server/src/api.ts`
- Im POST `/audits` Endpunkt:
  - Akzeptiere neues Feld `url_limit` aus Request-Body
  - Validiere: muss positive Zahl oder undefined sein
  - Übergebe an `startAudit()`

#### 3. Service-Änderung: `server/src/services/audit-service.ts`
- Funktion `startAudit()`:
  - Neuer Parameter: `urlLimit?: number`
  - Nach dem Parsen der Sitemap: Limitiere `urls` Array auf `urlLimit`
  - Speichere `url_limit` im Audit-Record
  - Setze `total_urls` auf die limitierte Anzahl (nicht die ursprüngliche Anzahl aus Sitemap)
  - Logging: "Found X URLs, limiting to Y URLs"

#### 4. UI-Änderung: `ui/src/pages/NewAudit.tsx`
- **Neues Formular-Feld:**
  - Label: "URL-Limit (optional)"
  - Dropdown/Select mit Optionen:
    - "Alle URLs" (default, value: undefined)
    - "10 URLs" (value: 10)
    - "25 URLs" (value: 25)
    - "50 URLs" (value: 50)
    - "100 URLs" (value: 100)
    - "250 URLs" (value: 250)
  - Beschreibung: "Limitiere die Anzahl der zu crawlenden URLs aus der Sitemap (nützlich zum Testen)"
  
- **State hinzufügen:**
  ```typescript
  const [urlLimit, setUrlLimit] = useState<number | undefined>(undefined);
  ```

- **API-Call anpassen:**
  ```typescript
  await api.createAudit({
    sitemap_url: sitemapUrl.trim(),
    rate_limit_ms: rateLimit,
    url_limit: urlLimit,
  });
  ```

#### 5. Type-Änderungen: `ui/src/lib/serverComm.ts`
- Interface `CreateAuditRequest`:
  ```typescript
  url_limit?: number;
  ```
- Interface `Audit`:
  ```typescript
  url_limit: number | null;
  ```

---

## Implementierungs-Schritte

### Phase 1: Database Schema
1. Erstelle Migration `0003_url_limit.sql`
2. Aktualisiere Schema-Definition in `audits.ts`
3. Führe Migration aus

### Phase 2: Backend (kann parallel zu Phase 3)
1. Aktualisiere `audit-service.ts` - `startAudit()` Funktion
2. Aktualisiere `api.ts` - POST `/audits` Endpunkt
3. Aktualisiere `job-queue.ts` - Multi-Worker Processor

### Phase 3: Frontend (kann parallel zu Phase 2)
1. Aktualisiere `serverComm.ts` - Types
2. Aktualisiere `NewAudit.tsx` - URL-Limit Formular-Feld
3. Aktualisiere `audit-table.tsx` - Status-Spalte und Sortierung

---

## Algorithmus: Multi-Worker Job-Processor

```
INITIALISIERUNG:
- Definiere WORKER_COUNT = 3
- Erstelle Map: activeCrawlsByAudit = {}

WORKER_LOOP(workerId):
  WHILE true:
    job = getNextJob()
    
    IF job == null:
      SLEEP(2000ms)
      CONTINUE
    
    IF job.type == "crawl_page":
      auditId = job.payload.audit_id
      
      // Rate-Limiting: Nur ein Crawl pro Audit gleichzeitig
      WHILE activeCrawlsByAudit[auditId] == true:
        SLEEP(500ms)
      
      activeCrawlsByAudit[auditId] = true
      
      TRY:
        processCrawlPageJob(job)
        completeJob(job.id)
      FINALLY:
        activeCrawlsByAudit[auditId] = false
    
    ELSE IF job.type == "analyze_page":
      // Analyze-Jobs können parallel laufen
      TRY:
        processAnalyzePageJob(job)
        completeJob(job.id)
      CATCH error:
        failJob(job.id, error)

MAIN:
  FOR i = 1 TO WORKER_COUNT:
    SPAWN WORKER_LOOP(i)
```

---

## Algorithmus: URL-Limiting

```
FUNKTION startAudit(sitemapUrl, userId, rateLimitMs, urlLimit):
  // Parse Sitemap
  urls = parseSitemap(sitemapUrl)
  originalCount = urls.length
  
  // Apply limit if specified
  IF urlLimit != null AND urlLimit > 0:
    urls = urls.slice(0, urlLimit)
    LOG("Found " + originalCount + " URLs, limiting to " + urls.length + " URLs")
  ELSE:
    LOG("Found " + originalCount + " URLs")
  
  // Rest der Funktion bleibt gleich
  // total_urls = urls.length (die limitierte Anzahl)
  ...
```

---

## Testing

### Job-Processor Test:
1. Starte Audit mit 100 URLs
2. Beobachte Logs: Mehrere Worker sollten parallel arbeiten
3. Überprüfe, dass Crawl-Jobs pro Audit sequenziell, Analyze-Jobs parallel laufen

### URL-Limit Test:
1. Erstelle Audit mit 10 URLs Limit aus großer Sitemap
2. Verifiziere in DB: `total_urls = 10`, `url_limit = 10`
3. Verifiziere: Nur 10 Page-Records werden erstellt

### UI Test:
1. Öffne AuditDetail für laufenden Audit
2. Beobachte: Status-Spalte zeigt aktuelle Status
3. Beobachte: Fortschritt aktualisiert sich alle 5 Sekunden
4. Verifiziere: Completed Seiten werden oben angezeigt

